{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from scipy.linalg import toeplitz\n",
    "from typing import NamedTuple, Tuple\n",
    "\n",
    "# Set JAX to use double precision for better numerical accuracy\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "class NUTSState(NamedTuple):\n",
    "    \"\"\"State for NUTS tree building\"\"\"\n",
    "    theta_minus: jnp.ndarray\n",
    "    theta_plus: jnp.ndarray\n",
    "    p_minus: jnp.ndarray\n",
    "    p_plus: jnp.ndarray\n",
    "    theta_sample: jnp.ndarray\n",
    "    n_valid: int\n",
    "    s: int\n",
    "    alpha: float\n",
    "    n_alpha: int\n",
    "\n",
    "\n",
    "class JAXNUTSDeblurring:\n",
    "    \"\"\"\n",
    "    NUTS sampler for Bayesian deblurring using JAX\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, A_matrix, y_obs, sigma, L_matrix, gamma, initial_x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        A_matrix : array\n",
    "            Forward operator (observation matrix)\n",
    "        y_obs : array\n",
    "            Observed data\n",
    "        sigma : float\n",
    "            Observation noise standard deviation\n",
    "        L_matrix : array\n",
    "            Prior precision matrix factor\n",
    "        gamma : float\n",
    "            Prior parameter\n",
    "        initial_x : array\n",
    "            Initial parameter guess\n",
    "        \"\"\"\n",
    "        # Convert to JAX arrays\n",
    "        self.A = jnp.array(A_matrix)\n",
    "        self.y_obs = jnp.array(y_obs)\n",
    "        self.sigma = sigma\n",
    "        self.L = jnp.array(L_matrix)\n",
    "        self.gamma = gamma\n",
    "        self.x_init = jnp.array(initial_x)\n",
    "        \n",
    "        # NUTS parameters\n",
    "        self.max_tree_depth = 10\n",
    "        self.target_accept = 0.8\n",
    "        self.Delta_max = 1000  # Maximum allowed energy difference\n",
    "        \n",
    "        # Mass matrix (will be adapted)\n",
    "        self.mass_matrix = jnp.eye(len(initial_x))\n",
    "        self.mass_matrix_inv = jnp.eye(len(initial_x))\n",
    "        \n",
    "        # Compile functions\n",
    "        self._compile_functions()\n",
    "        \n",
    "    def _compile_functions(self):\n",
    "        \"\"\"Compile JAX functions for efficiency\"\"\"\n",
    "        \n",
    "        @jit\n",
    "        def forward_operator(x):\n",
    "            \"\"\"Forward operator: x -> Ax\"\"\"\n",
    "            return self.A @ x\n",
    "        \n",
    "        @jit\n",
    "        def log_likelihood(x):\n",
    "            \"\"\"Compute log-likelihood\"\"\"\n",
    "            predictions = forward_operator(x)\n",
    "            residual = predictions - self.y_obs\n",
    "            return -0.5 * jnp.sum(residual**2) / self.sigma**2\n",
    "        \n",
    "        @jit\n",
    "        def log_prior(x):\n",
    "            \"\"\"Compute log-prior using precision matrix\"\"\"\n",
    "            Lx = self.L @ x\n",
    "            return -0.5 * jnp.sum(Lx**2) / self.gamma**2\n",
    "        \n",
    "        @jit\n",
    "        def log_posterior(x):\n",
    "            \"\"\"Compute log-posterior\"\"\"\n",
    "            return log_likelihood(x) + log_prior(x)\n",
    "        \n",
    "        # Store compiled functions\n",
    "        self.forward_operator = forward_operator\n",
    "        self.log_posterior_fn = log_posterior\n",
    "        self.grad_log_posterior_fn = jit(grad(log_posterior))\n",
    "        \n",
    "        # Combined function for efficiency\n",
    "        @jit\n",
    "        def log_posterior_and_grad(x):\n",
    "            return log_posterior(x), grad(log_posterior)(x)\n",
    "        \n",
    "        self.log_posterior_and_grad_fn = log_posterior_and_grad\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def leapfrog_step(self, theta, p, epsilon):\n",
    "        \"\"\"Single leapfrog integration step\"\"\"\n",
    "        # Half step for momentum\n",
    "        _, grad_theta = self.log_posterior_and_grad_fn(theta)\n",
    "        p = p + 0.5 * epsilon * grad_theta\n",
    "        \n",
    "        # Full step for position\n",
    "        theta = theta + epsilon * self.mass_matrix_inv @ p\n",
    "        \n",
    "        # Half step for momentum\n",
    "        _, grad_theta = self.log_posterior_and_grad_fn(theta)\n",
    "        p = p + 0.5 * epsilon * grad_theta\n",
    "        \n",
    "        return theta, p\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_hamiltonian(self, theta, p):\n",
    "        \"\"\"Compute Hamiltonian (total energy)\"\"\"\n",
    "        log_p = self.log_posterior_fn(theta)\n",
    "        kinetic = 0.5 * p @ self.mass_matrix_inv @ p\n",
    "        return -log_p + kinetic\n",
    "    \n",
    "    def find_reasonable_epsilon(self, theta, key):\n",
    "        \"\"\"Find reasonable initial step size (from Stan)\"\"\"\n",
    "        # Sample momentum\n",
    "        key, subkey = random.split(key)\n",
    "        p = random.multivariate_normal(subkey, jnp.zeros(len(theta)), self.mass_matrix)\n",
    "        \n",
    "        # Initial step\n",
    "        epsilon = 1.0\n",
    "        theta_new, p_new = self.leapfrog_step(theta, p, epsilon)\n",
    "        \n",
    "        # Compute acceptance probability\n",
    "        H_old = self.compute_hamiltonian(theta, p)\n",
    "        H_new = self.compute_hamiltonian(theta_new, p_new)\n",
    "        \n",
    "        a = 2.0 * (jnp.exp(H_old - H_new) > 0.5) - 1.0\n",
    "        \n",
    "        # Scale epsilon until acceptance probability crosses 0.5\n",
    "        while (jnp.exp(a * (H_old - H_new)) ** a) > (2.0 ** (-a)):\n",
    "            epsilon = epsilon * (2.0 ** a)\n",
    "            theta_new, p_new = self.leapfrog_step(theta, p, epsilon)\n",
    "            H_new = self.compute_hamiltonian(theta_new, p_new)\n",
    "        \n",
    "        return epsilon\n",
    "    \n",
    "    def stop_criterion(self, theta_minus, theta_plus, p_minus, p_plus):\n",
    "        \"\"\"Check U-turn condition\"\"\"\n",
    "        span = theta_plus - theta_minus\n",
    "        # U-turn if trajectory starts going backwards\n",
    "        return (jnp.dot(span, p_minus) >= 0) & (jnp.dot(span, p_plus) >= 0)\n",
    "    \n",
    "    def build_tree(self, theta, p, u, v, j, epsilon, key):\n",
    "        \"\"\"\n",
    "        Recursively build binary tree for NUTS\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Tree state and key\n",
    "        \"\"\"\n",
    "        if j == 0:\n",
    "            # Base case: single leapfrog step\n",
    "            theta_prime, p_prime = self.leapfrog_step(theta, p, v * epsilon)\n",
    "            \n",
    "            # Compute Hamiltonian\n",
    "            H = self.compute_hamiltonian(theta_prime, p_prime)\n",
    "            \n",
    "            # Check if in slice\n",
    "            n_prime = (u <= jnp.exp(-H)).astype(jnp.int32)\n",
    "            s_prime = (u < jnp.exp(self.Delta_max - H)).astype(jnp.int32)\n",
    "            \n",
    "            # Acceptance probability\n",
    "            alpha = jnp.minimum(1.0, jnp.exp(-H + self.compute_hamiltonian(theta, p)))\n",
    "            \n",
    "            return NUTSState(\n",
    "                theta_minus=theta_prime,\n",
    "                theta_plus=theta_prime,\n",
    "                p_minus=p_prime,\n",
    "                p_plus=p_prime,\n",
    "                theta_sample=theta_prime,\n",
    "                n_valid=n_prime,\n",
    "                s=s_prime,\n",
    "                alpha=alpha,\n",
    "                n_alpha=1\n",
    "            ), key\n",
    "        \n",
    "        else:\n",
    "            # Recursion: build first subtree\n",
    "            tree, key = self.build_tree(theta, p, u, v, j - 1, epsilon, key)\n",
    "            \n",
    "            # If first subtree didn't fail, build second\n",
    "            if tree.s == 1:\n",
    "                if v == -1:\n",
    "                    # Build tree to the left\n",
    "                    tree2, key = self.build_tree(\n",
    "                        tree.theta_minus, tree.p_minus, u, v, j - 1, epsilon, key\n",
    "                    )\n",
    "                    theta_minus = tree2.theta_minus\n",
    "                    p_minus = tree2.p_minus\n",
    "                    theta_plus = tree.theta_plus\n",
    "                    p_plus = tree.p_plus\n",
    "                else:\n",
    "                    # Build tree to the right\n",
    "                    tree2, key = self.build_tree(\n",
    "                        tree.theta_plus, tree.p_plus, u, v, j - 1, epsilon, key\n",
    "                    )\n",
    "                    theta_minus = tree.theta_minus\n",
    "                    p_minus = tree.p_minus\n",
    "                    theta_plus = tree2.theta_plus\n",
    "                    p_plus = tree2.p_plus\n",
    "                \n",
    "                # Metropolis step for sampling\n",
    "                key, subkey = random.split(key)\n",
    "                accept = random.uniform(subkey) < (tree2.n_valid / (tree.n_valid + tree2.n_valid))\n",
    "                theta_sample = jnp.where(accept, tree2.theta_sample, tree.theta_sample)\n",
    "                \n",
    "                # Update acceptance statistics\n",
    "                alpha = tree.alpha + tree2.alpha\n",
    "                n_alpha = tree.n_alpha + tree2.n_alpha\n",
    "                \n",
    "                # Check stopping criterion\n",
    "                s = tree2.s * self.stop_criterion(theta_minus, theta_plus, p_minus, p_plus).astype(jnp.int32)\n",
    "                n_valid = tree.n_valid + tree2.n_valid\n",
    "                \n",
    "                return NUTSState(\n",
    "                    theta_minus=theta_minus,\n",
    "                    theta_plus=theta_plus,\n",
    "                    p_minus=p_minus,\n",
    "                    p_plus=p_plus,\n",
    "                    theta_sample=theta_sample,\n",
    "                    n_valid=n_valid,\n",
    "                    s=s,\n",
    "                    alpha=alpha,\n",
    "                    n_alpha=n_alpha\n",
    "                ), key\n",
    "            else:\n",
    "                return tree, key\n",
    "    \n",
    "    def nuts_step(self, theta_current, epsilon, key):\n",
    "        \"\"\"Single NUTS transition\"\"\"\n",
    "        # Sample momentum\n",
    "        key, subkey = random.split(key)\n",
    "        p = random.multivariate_normal(subkey, jnp.zeros(len(theta_current)), self.mass_matrix)\n",
    "        \n",
    "        # Sample slice variable\n",
    "        key, subkey = random.split(key)\n",
    "        u = random.uniform(subkey) * jnp.exp(-self.compute_hamiltonian(theta_current, p))\n",
    "        \n",
    "        # Initialize tree\n",
    "        theta_minus = theta_current.copy()\n",
    "        theta_plus = theta_current.copy()\n",
    "        p_minus = p.copy()\n",
    "        p_plus = p.copy()\n",
    "        \n",
    "        j = 0  # Tree depth\n",
    "        theta_next = theta_current.copy()\n",
    "        n = 1\n",
    "        s = 1\n",
    "        \n",
    "        alpha_sum = 0.0\n",
    "        n_alpha_sum = 0\n",
    "        \n",
    "        # Build tree until U-turn or max depth\n",
    "        while (s == 1) & (j < self.max_tree_depth):\n",
    "            # Choose direction uniformly\n",
    "            key, subkey = random.split(key)\n",
    "            v = 2 * (random.uniform(subkey) < 0.5) - 1\n",
    "            \n",
    "            if v == -1:\n",
    "                # Build tree to the left\n",
    "                tree, key = self.build_tree(theta_minus, p_minus, u, v, j, epsilon, key)\n",
    "                theta_minus = tree.theta_minus\n",
    "                p_minus = tree.p_minus\n",
    "            else:\n",
    "                # Build tree to the right\n",
    "                tree, key = self.build_tree(theta_plus, p_plus, u, v, j, epsilon, key)\n",
    "                theta_plus = tree.theta_plus\n",
    "                p_plus = tree.p_plus\n",
    "            \n",
    "            # Metropolis sampling from the tree\n",
    "            if tree.s == 1:\n",
    "                key, subkey = random.split(key)\n",
    "                accept = random.uniform(subkey) < (tree.n_valid / n)\n",
    "                theta_next = jnp.where(accept, tree.theta_sample, theta_next)\n",
    "            \n",
    "            # Update statistics\n",
    "            alpha_sum += tree.alpha\n",
    "            n_alpha_sum += tree.n_alpha\n",
    "            \n",
    "            # Update number of valid points\n",
    "            n = n + tree.n_valid\n",
    "            \n",
    "            # Check for U-turn\n",
    "            s = tree.s * self.stop_criterion(theta_minus, theta_plus, p_minus, p_plus).astype(jnp.int32)\n",
    "            \n",
    "            j += 1\n",
    "        \n",
    "        # Average acceptance probability\n",
    "        avg_alpha = alpha_sum / jnp.maximum(n_alpha_sum, 1)\n",
    "        \n",
    "        return theta_next, avg_alpha, j, key\n",
    "    \n",
    "    def adapt_step_size(self, epsilon, H_bar, iteration, avg_alpha):\n",
    "        \"\"\"Dual averaging for step size adaptation\"\"\"\n",
    "        # Parameters for dual averaging\n",
    "        gamma = 0.05\n",
    "        t0 = 10.0\n",
    "        kappa = 0.75\n",
    "        mu = jnp.log(10.0 * epsilon)\n",
    "        \n",
    "        # Update H_bar\n",
    "        eta = 1.0 / (iteration + t0)\n",
    "        H_bar = (1.0 - eta) * H_bar + eta * (self.target_accept - avg_alpha)\n",
    "        \n",
    "        # Update log epsilon\n",
    "        log_epsilon = mu - jnp.sqrt(iteration) / gamma * H_bar\n",
    "        \n",
    "        # Update epsilon with damping\n",
    "        epsilon = jnp.exp(log_epsilon)\n",
    "        \n",
    "        # Also compute smoothed version\n",
    "        epsilon_bar = jnp.exp(\n",
    "            iteration ** (-kappa) * log_epsilon + \n",
    "            (1 - iteration ** (-kappa)) * jnp.log(epsilon)\n",
    "        )\n",
    "        \n",
    "        return epsilon, epsilon_bar, H_bar\n",
    "    \n",
    "    def sample(self, n_samples, n_warmup=1000, seed=42):\n",
    "        \"\"\"\n",
    "        Run NUTS sampling with adaptation\n",
    "        \"\"\"\n",
    "        key = random.PRNGKey(seed)\n",
    "        theta = self.x_init.copy()\n",
    "        \n",
    "        # Find reasonable initial step size\n",
    "        print(\"Finding reasonable initial step size...\")\n",
    "        key, subkey = random.split(key)\n",
    "        epsilon = self.find_reasonable_epsilon(theta, subkey)\n",
    "        epsilon_bar = epsilon\n",
    "        print(f\"Initial epsilon: {epsilon:.4f}\")\n",
    "        \n",
    "        # Storage\n",
    "        samples = []\n",
    "        accept_probs = []\n",
    "        tree_depths = []\n",
    "        epsilons = []\n",
    "        \n",
    "        # Adaptation parameters\n",
    "        H_bar = 0.0\n",
    "        mu = jnp.log(10.0 * epsilon)\n",
    "        \n",
    "        print(\"Starting warmup phase...\")\n",
    "        warmup_samples = []\n",
    "        \n",
    "        # Combined warmup and sampling\n",
    "        total_iterations = n_warmup + n_samples\n",
    "        \n",
    "        for i in range(total_iterations):\n",
    "            # NUTS step\n",
    "            theta, avg_alpha, tree_depth, key = self.nuts_step(theta, epsilon, key)\n",
    "            \n",
    "            # During warmup\n",
    "            if i < n_warmup:\n",
    "                # Adapt step size\n",
    "                iteration = i + 1\n",
    "                epsilon, epsilon_bar, H_bar = self.adapt_step_size(\n",
    "                    epsilon, H_bar, iteration, avg_alpha\n",
    "                )\n",
    "                \n",
    "                # Store for mass matrix adaptation (second half of warmup)\n",
    "                if i > n_warmup // 2:\n",
    "                    warmup_samples.append(theta)\n",
    "                \n",
    "                # Adapt mass matrix periodically\n",
    "                if (i + 1) % 100 == 0 and len(warmup_samples) > 50:\n",
    "                    # Compute sample covariance\n",
    "                    warmup_array = jnp.array(warmup_samples)\n",
    "                    sample_cov = jnp.cov(warmup_array.T)\n",
    "                    \n",
    "                    # Regularize for stability\n",
    "                    reg = 1e-6 * jnp.trace(sample_cov) / len(sample_cov)\n",
    "                    self.mass_matrix = sample_cov + reg * jnp.eye(len(sample_cov))\n",
    "                    self.mass_matrix_inv = jnp.linalg.inv(self.mass_matrix)\n",
    "                \n",
    "                # Use smoothed epsilon during warmup\n",
    "                if i < n_warmup - 100:\n",
    "                    epsilon = epsilon_bar\n",
    "                \n",
    "                # Progress report\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    print(f\"Warmup {i+1}/{n_warmup}, \"\n",
    "                          f\"avg_alpha: {avg_alpha:.3f}, \"\n",
    "                          f\"epsilon: {epsilon:.4f}, \"\n",
    "                          f\"tree_depth: {tree_depth}\")\n",
    "            \n",
    "            # After warmup - collect samples\n",
    "            else:\n",
    "                samples.append(theta)\n",
    "                accept_probs.append(avg_alpha)\n",
    "                tree_depths.append(tree_depth)\n",
    "                epsilons.append(epsilon)\n",
    "                \n",
    "                # Progress report\n",
    "                if (i + 1 - n_warmup) % 100 == 0:\n",
    "                    current_iter = i + 1 - n_warmup\n",
    "                    avg_accept = np.mean(accept_probs)\n",
    "                    avg_depth = np.mean(tree_depths)\n",
    "                    print(f\"Sample {current_iter}/{n_samples}, \"\n",
    "                          f\"avg_accept: {avg_accept:.3f}, \"\n",
    "                          f\"avg_tree_depth: {avg_depth:.1f}\")\n",
    "        \n",
    "        samples = jnp.array(samples)\n",
    "        \n",
    "        # Final adaptation of mass matrix from all warmup samples\n",
    "        if len(warmup_samples) > 100:\n",
    "            print(f\"\\nFinal mass matrix adaptation using {len(warmup_samples)} warmup samples\")\n",
    "            warmup_array = jnp.array(warmup_samples)\n",
    "            sample_cov = jnp.cov(warmup_array.T)\n",
    "            reg = 1e-6 * jnp.trace(sample_cov) / len(sample_cov)\n",
    "            self.mass_matrix = sample_cov + reg * jnp.eye(len(sample_cov))\n",
    "            self.mass_matrix_inv = jnp.linalg.inv(self.mass_matrix)\n",
    "        \n",
    "        # Compute diagnostics\n",
    "        avg_accept = np.mean(accept_probs)\n",
    "        avg_tree_depth = np.mean(tree_depths)\n",
    "        max_tree_depth = np.max(tree_depths)\n",
    "        \n",
    "        print(f\"\\nSampling completed!\")\n",
    "        print(f\"Average acceptance probability: {avg_accept:.3f}\")\n",
    "        print(f\"Average tree depth: {avg_tree_depth:.1f}\")\n",
    "        print(f\"Maximum tree depth: {max_tree_depth}\")\n",
    "        print(f\"Final epsilon: {epsilon:.4f}\")\n",
    "        \n",
    "        # Diagnostics\n",
    "        diagnostics = {\n",
    "            'accept_rate': avg_accept,\n",
    "            'accept_probs': jnp.array(accept_probs),\n",
    "            'tree_depths': jnp.array(tree_depths),\n",
    "            'epsilons': jnp.array(epsilons),\n",
    "            'final_epsilon': epsilon,\n",
    "            'final_mass_matrix': self.mass_matrix,\n",
    "            'avg_tree_depth': avg_tree_depth,\n",
    "            'max_tree_depth': max_tree_depth\n",
    "        }\n",
    "        \n",
    "        return samples, diagnostics\n",
    "    \n",
    "    def posterior_predictive(self, samples, n_pred=None):\n",
    "        \"\"\"Generate posterior predictive samples\"\"\"\n",
    "        if n_pred is None:\n",
    "            n_pred = len(samples)\n",
    "        \n",
    "        # Select random subset of samples\n",
    "        key = random.PRNGKey(0)\n",
    "        indices = random.choice(key, len(samples), shape=(n_pred,), replace=True)\n",
    "        selected_samples = samples[indices]\n",
    "        \n",
    "        # Vectorized forward operator\n",
    "        predictions = vmap(self.forward_operator)(selected_samples)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "def compute_analytical_posterior(A, y_obs, sigma, L, gamma):\n",
    "    \"\"\"\n",
    "    Compute the analytical posterior for linear Gaussian inverse problem\n",
    "    \"\"\"\n",
    "    from scipy.linalg import inv\n",
    "    \n",
    "    # Posterior precision matrix\n",
    "    precision_post = (1/sigma**2) * A.T @ A + (1/gamma**2) * L.T @ L\n",
    "    \n",
    "    # Posterior covariance matrix\n",
    "    cov_post = inv(precision_post)\n",
    "    \n",
    "    # Posterior mean\n",
    "    mean_post = cov_post @ ((1/sigma**2) * A.T @ y_obs)\n",
    "    \n",
    "    return mean_post, cov_post\n",
    "\n",
    "\n",
    "def plot_nuts_vs_analytical(t, nuts_samples, xtrue, t_obs, y_obs, diagnostics, \n",
    "                            analytical_mean, analytical_cov):\n",
    "    \"\"\"Plot comparison between NUTS and analytical results\"\"\"\n",
    "    \n",
    "    # Compute NUTS statistics\n",
    "    nuts_mean = np.mean(nuts_samples, axis=0)\n",
    "    nuts_std = np.std(nuts_samples, axis=0)\n",
    "    nuts_q025 = np.percentile(nuts_samples, 2.5, axis=0)\n",
    "    nuts_q975 = np.percentile(nuts_samples, 97.5, axis=0)\n",
    "    \n",
    "    # Compute analytical statistics\n",
    "    analytical_std = np.sqrt(np.diag(analytical_cov))\n",
    "    analytical_q025 = analytical_mean - 1.96 * analytical_std\n",
    "    analytical_q975 = analytical_mean + 1.96 * analytical_std\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    \n",
    "    # Main reconstruction comparison\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    # NUTS uncertainty\n",
    "    t_fill = np.concatenate([t, t[::-1]])\n",
    "    nuts_fill = np.concatenate([nuts_q025, nuts_q975[::-1]])\n",
    "    ax.fill(t_fill, nuts_fill, color='lightblue', alpha=0.5, label='NUTS 95% CI')\n",
    "    \n",
    "    # Analytical uncertainty\n",
    "    analytical_fill = np.concatenate([analytical_q025, analytical_q975[::-1]])\n",
    "    ax.fill(t_fill, analytical_fill, color='lightcoral', alpha=0.5, label='Analytical 95% CI')\n",
    "    \n",
    "    # Means\n",
    "    ax.plot(t, nuts_mean, 'b-', linewidth=2, label='NUTS Mean')\n",
    "    ax.plot(t, analytical_mean, 'r--', linewidth=2, label='Analytical Mean')\n",
    "    ax.plot(t, xtrue, 'k-', linewidth=1.5, label='Truth')\n",
    "    ax.plot(t_obs, y_obs, 'go', markersize=6, label='Observations')\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('x(t)')\n",
    "    ax.set_title('NUTS vs Analytical Posterior')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mean comparison\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(analytical_mean, nuts_mean, 'b.', alpha=0.6)\n",
    "    ax.plot([analytical_mean.min(), analytical_mean.max()], \n",
    "            [analytical_mean.min(), analytical_mean.max()], 'r--', label='y=x')\n",
    "    ax.set_xlabel('Analytical Mean')\n",
    "    ax.set_ylabel('NUTS Mean')\n",
    "    ax.set_title('Posterior Mean Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Standard deviation comparison\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(analytical_std, nuts_std, 'b.', alpha=0.6)\n",
    "    ax.plot([analytical_std.min(), analytical_std.max()], \n",
    "            [analytical_std.min(), analytical_std.max()], 'r--', label='y=x')\n",
    "    ax.set_xlabel('Analytical Std')\n",
    "    ax.set_ylabel('NUTS Std')\n",
    "    ax.set_title('Posterior Std Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error in mean\n",
    "    ax = axes[1, 0]\n",
    "    mean_error = np.abs(nuts_mean - analytical_mean)\n",
    "    ax.plot(t, mean_error, 'r-', linewidth=2)\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('|NUTS Mean - Analytical Mean|')\n",
    "    ax.set_title('Absolute Error in Posterior Mean')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Error in standard deviation\n",
    "    ax = axes[1, 1]\n",
    "    std_error = np.abs(nuts_std - analytical_std)\n",
    "    ax.plot(t, std_error, 'b-', linewidth=2)\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('|NUTS Std - Analytical Std|')\n",
    "    ax.set_title('Absolute Error in Posterior Std')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Tree depth over iterations\n",
    "    ax = axes[1, 2]\n",
    "    ax.plot(diagnostics['tree_depths'], alpha=0.7)\n",
    "    ax.axhline(y=diagnostics['avg_tree_depth'], color='red', linestyle='--', \n",
    "               label=f\"Average: {diagnostics['avg_tree_depth']:.1f}\")\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Tree Depth')\n",
    "    ax.set_title('NUTS Tree Depth')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Acceptance probability\n",
    "    ax = axes[2, 0]\n",
    "    ax.plot(diagnostics['accept_probs'], alpha=0.7)\n",
    "    ax.axhline(y=diagnostics['accept_rate'], color='red', linestyle='--', \n",
    "               label=f\"Average: {diagnostics['accept_rate']:.3f}\")\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Acceptance Probability')\n",
    "    ax.set_title('NUTS Acceptance Rate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Step size evolution\n",
    "    ax = axes[2, 1]\n",
    "    ax.plot(diagnostics['epsilons'], alpha=0.7)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Step Size (epsilon)')\n",
    "    ax.set_title('NUTS Step Size')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Tree depth histogram\n",
    "    ax = axes[2, 2]\n",
    "    ax.hist(diagnostics['tree_depths'], bins=np.arange(0, diagnostics['max_tree_depth']+2)-0.5, \n",
    "            edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Tree Depth')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Tree Depth Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical comparisons\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NUMERICAL COMPARISON: NUTS vs Analytical\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Mean comparison\n",
    "    mean_rmse = np.sqrt(np.mean((nuts_mean - analytical_mean)**2))\n",
    "    mean_max_error = np.max(np.abs(nuts_mean - analytical_mean))\n",
    "    print(f\"Posterior Mean:\")\n",
    "    print(f\"  RMSE: {mean_rmse:.6e}\")\n",
    "    print(f\"  Max absolute error: {mean_max_error:.6e}\")\n",
    "    print(f\"  Relative RMSE: {mean_rmse/np.std(analytical_mean):.6e}\")\n",
    "    \n",
    "    # Standard deviation comparison\n",
    "    std_rmse = np.sqrt(np.mean((nuts_std - analytical_std)**2))\n",
    "    std_max_error = np.max(np.abs(nuts_std - analytical_std))\n",
    "    print(f\"\\nPosterior Standard Deviation:\")\n",
    "    print(f\"  RMSE: {std_rmse:.6e}\")\n",
    "    print(f\"  Max absolute error: {std_max_error:.6e}\")\n",
    "    print(f\"  Relative RMSE: {std_rmse/np.mean(analytical_std):.6e}\")\n",
    "    \n",
    "    # Tree depth statistics\n",
    "    print(f\"\\nNUTS Tree Depth Statistics:\")\n",
    "    print(f\"  Mean: {diagnostics['avg_tree_depth']:.1f}\")\n",
    "    print(f\"  Max: {diagnostics['max_tree_depth']}\")\n",
    "    print(f\"  Min: {np.min(diagnostics['tree_depths'])}\")\n",
    "    print(f\"  Std: {np.std(diagnostics['tree_depths']):.2f}\")\n",
    "\n",
    "\n",
    "def effective_sample_size(samples):\n",
    "    \"\"\"Compute effective sample size for each parameter\"\"\"\n",
    "    from scipy import signal\n",
    "    \n",
    "    def ess_single_chain(x):\n",
    "        \"\"\"ESS for a single chain\"\"\"\n",
    "        n = len(x)\n",
    "        x = x - np.mean(x)\n",
    "        \n",
    "        # Auto-correlation function\n",
    "        autocorr = signal.correlate(x, x, mode='full')\n",
    "        autocorr = autocorr[n-1:]\n",
    "        autocorr = autocorr / autocorr[0]\n",
    "        \n",
    "        # Find first negative autocorrelation\n",
    "        first_negative = np.where(autocorr < 0)[0]\n",
    "        if len(first_negative) > 0:\n",
    "            cutoff = first_negative[0]\n",
    "        else:\n",
    "            cutoff = len(autocorr)\n",
    "        \n",
    "        # Sum autocorrelations up to cutoff\n",
    "        sum_autocorr = 2 * np.sum(autocorr[1:cutoff]) + 1\n",
    "        \n",
    "        return n / sum_autocorr if sum_autocorr > 0 else n\n",
    "    \n",
    "    # Compute ESS for each parameter\n",
    "    ess_values = []\n",
    "    for i in range(samples.shape[1]):\n",
    "        ess_values.append(ess_single_chain(samples[:, i]))\n",
    "    \n",
    "    return np.array(ess_values)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the NUTS deblurring example\"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility  \n",
    "    np.random.seed(20)\n",
    "    \n",
    "    # Setup problem\n",
    "    n = 100\n",
    "    s = np.linspace(0, 1, n+1)\n",
    "    t = s.copy()\n",
    "    \n",
    "    # Sparse observation settings\n",
    "    n_obs = 15\n",
    "    obs_indices = np.sort(np.random.choice(n+1, n_obs, replace=False))\n",
    "    t_obs = t[obs_indices]\n",
    "    \n",
    "    # Prior settings\n",
    "    PriorFlag = 2\n",
    "    \n",
    "    # Discretize the deblurring kernel\n",
    "    beta = 0.05\n",
    "    a = (1/np.sqrt(2*np.pi*beta**2)) * np.exp(-0.5*(1/beta**2)*t**2)\n",
    "    A_full = (1/n) * toeplitz(a)\n",
    "    A = A_full[obs_indices, :]  # Sparse observation matrix\n",
    "    \n",
    "    # Truth\n",
    "    xtrue = 10*(t-0.5)*np.exp(-0.5*1e2*(t-0.5)**2) - 0.8 + 1.6*t\n",
    "    \n",
    "    # Generate observations with noise\n",
    "    noise = 5\n",
    "    y0_full = A_full @ xtrue\n",
    "    y0 = y0_full[obs_indices]\n",
    "    sigma = np.max(np.abs(y0_full)) * noise / 100\n",
    "    y_obs = y0 + sigma * np.random.randn(n_obs)\n",
    "    \n",
    "    # Prior construction\n",
    "    gamma = 1/n\n",
    "    \n",
    "    if PriorFlag == 1:\n",
    "        L = (np.diag(np.ones(n+1)) - \n",
    "             np.diag(0.5*np.ones(n), 1) - \n",
    "             np.diag(0.5*np.ones(n), -1))\n",
    "    elif PriorFlag == 2:\n",
    "        from scipy.linalg import inv\n",
    "        L_D = (np.diag(np.ones(n+1)) - \n",
    "               np.diag(0.5*np.ones(n), 1) - \n",
    "               np.diag(0.5*np.ones(n), -1))\n",
    "        \n",
    "        L_Dinv = inv(L_D)\n",
    "        Dev = np.sqrt(gamma**2 * np.diag(L_Dinv @ L_Dinv.T))\n",
    "        \n",
    "        delta = gamma / Dev[n//2]\n",
    "        L = L_D.copy()\n",
    "        L[0, :] = 0\n",
    "        L[0, 0] = delta\n",
    "        L[-1, :] = 0\n",
    "        L[-1, -1] = delta\n",
    "    \n",
    "    # Compute analytical posterior solution\n",
    "    print(\"Computing analytical posterior...\")\n",
    "    analytical_mean, analytical_cov = compute_analytical_posterior(A, y_obs, sigma, L, gamma)\n",
    "    \n",
    "    # Get MAP estimate as initial point for NUTS (should equal analytical mean)\n",
    "    A_aug = np.vstack([(1/sigma)*A, (1/gamma)*L])\n",
    "    b_aug = np.concatenate([(1/sigma)*y_obs, np.zeros(n+1)])\n",
    "    x_map = np.linalg.lstsq(A_aug, b_aug, rcond=None)[0]\n",
    "    \n",
    "    print(\"Setting up NUTS sampler...\")\n",
    "    \n",
    "    # Initialize NUTS sampler\n",
    "    sampler = JAXNUTSDeblurring(\n",
    "        A_matrix=A,\n",
    "        y_obs=y_obs,\n",
    "        sigma=sigma,\n",
    "        L_matrix=L,\n",
    "        gamma=gamma,\n",
    "        initial_x=x_map  # Start from MAP estimate\n",
    "    )\n",
    "    \n",
    "    # Run NUTS sampling\n",
    "    print(\"Running NUTS sampling...\")\n",
    "    samples, diagnostics = sampler.sample(\n",
    "        n_samples=2000,  # Number of samples to collect\n",
    "        n_warmup=1000,   # Warmup iterations for adaptation\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nSampling completed!\")\n",
    "    print(f\"Final acceptance rate: {diagnostics['accept_rate']:.3f}\")\n",
    "    print(f\"Average tree depth: {diagnostics['avg_tree_depth']:.1f}\")\n",
    "    print(f\"Final step size: {diagnostics['final_epsilon']:.4f}\")\n",
    "    \n",
    "    # Compute NUTS posterior statistics\n",
    "    nuts_mean = np.mean(samples, axis=0)\n",
    "    nuts_std = np.std(samples, axis=0)\n",
    "    \n",
    "    print(f\"\\nFirst 5 elements comparison:\")\n",
    "    print(f\"Truth:           {xtrue[:5]}\")\n",
    "    print(f\"Analytical mean: {analytical_mean[:5]}\")\n",
    "    print(f\"NUTS mean:       {nuts_mean[:5]}\")\n",
    "    print(f\"MAP estimate:    {x_map[:5]}\")\n",
    "    \n",
    "    # Compute effective sample size\n",
    "    ess = effective_sample_size(samples)\n",
    "    print(f\"\\nEffective sample size statistics:\")\n",
    "    print(f\"  Mean ESS: {np.mean(ess):.1f}\")\n",
    "    print(f\"  Min ESS:  {np.min(ess):.1f}\")\n",
    "    print(f\"  Max ESS:  {np.max(ess):.1f}\")\n",
    "    print(f\"  ESS/iteration: {np.mean(ess)/len(samples):.2%}\")\n",
    "    \n",
    "    # Verification that MAP = analytical mean (should be very close)\n",
    "    map_analytical_error = np.max(np.abs(x_map - analytical_mean))\n",
    "    print(f\"\\nMAP vs Analytical mean max error: {map_analytical_error:.2e}\")\n",
    "    print(\"(Should be very small - confirms analytical solution)\")\n",
    "    \n",
    "    # Compare NUTS efficiency to standard HMC\n",
    "    print(f\"\\nNUTS Efficiency Metrics:\")\n",
    "    print(f\"  Total leapfrog steps: {np.sum(diagnostics['tree_depths'])}\")\n",
    "    print(f\"  Average leapfrog steps per sample: {diagnostics['avg_tree_depth']:.1f}\")\n",
    "    print(f\"  Compared to fixed HMC with L=15: {diagnostics['avg_tree_depth']/15:.2f}x\")\n",
    "    \n",
    "    # Plot comprehensive comparison\n",
    "    plot_nuts_vs_analytical(t, samples, xtrue, t_obs, y_obs, diagnostics, \n",
    "                            analytical_mean, analytical_cov)\n",
    "    \n",
    "    return samples, diagnostics, sampler, analytical_mean, analytical_cov\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    samples, diagnostics, sampler, analytical_mean, analytical_cov = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca71da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
