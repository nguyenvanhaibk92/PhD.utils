{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3405b4c3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "## A Tutorial on Score-Based Generative Model from Scratch with JaxAIStack \n",
    "### Hai Nguyen\n",
    "### The University of Texas at Austin\n",
    "### Aerospace Engineering and Engineering Mechanics\n",
    "#### 2025 June 30\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f9638",
   "metadata": {},
   "source": [
    "#### This notebook is a mixed version of theory and implementation of a transformer model. Materials for this notebook\n",
    "- Tutorial notebook on Score-based model: [Original GoogleColab notebook](https://colab.research.google.com/drive/1SeXMpILhkJPjXUaesvzEhc3Ke6Zl_zxJ?usp=sharing#scrollTo=LZC7wrOvxLdL)\n",
    "- The blog post on Score-based model: [Yang Song's blog](https://yang-song.net/blog/2021/score/)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b181d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e3ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9ac814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Defining a time-dependent score-based model (double click to expand or collapse)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from typing import Any, Tuple\n",
    "import functools\n",
    "import jax\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "  embed_dim: int\n",
    "  scale: float = 30.\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # Randomly sample weights during initialization. These weights are fixed\n",
    "    # during optimization and are not trainable.\n",
    "    W = self.param('W', jax.nn.initializers.normal(stddev=self.scale),\n",
    "                 (self.embed_dim // 2, ))\n",
    "    W = jax.lax.stop_gradient(W)\n",
    "    x_proj = x[:, None] * W[None, :] * 2 * jnp.pi\n",
    "    return jnp.concatenate([jnp.sin(x_proj), jnp.cos(x_proj)], axis=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "  output_dim: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    return nn.Dense(self.output_dim)(x)[:, None, None, :]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\n",
    "\n",
    "  Args:\n",
    "      marginal_prob_std: A function that takes time t and gives the standard\n",
    "        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "      channels: The number of channels for feature maps of each resolution.\n",
    "      embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "  \"\"\"\n",
    "  marginal_prob_std: Any\n",
    "  channels: Tuple[int] = (32, 64, 128, 256)\n",
    "  embed_dim: int = 256\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, t):\n",
    "    # The swish activation function\n",
    "    act = nn.swish\n",
    "    # Obtain the Gaussian random feature embedding for t\n",
    "    embed = act(nn.Dense(self.embed_dim)(\n",
    "        GaussianFourierProjection(embed_dim=self.embed_dim)(t)))\n",
    "\n",
    "    # Encoding path\n",
    "    h1 = nn.Conv(self.channels[0], (3, 3), (1, 1), padding='VALID', use_bias=False)(x)\n",
    "    ## Incorporate information from t\n",
    "    h1 += Dense(self.channels[0])(embed)\n",
    "    ## Group normalization\n",
    "    h1 = nn.GroupNorm(4)(h1)\n",
    "    h1 = act(h1)\n",
    "    h2 = nn.Conv(self.channels[1], (3, 3), (2, 2), padding='VALID', use_bias=False)(h1)\n",
    "    h2 += Dense(self.channels[1])(embed)\n",
    "    h2 = nn.GroupNorm()(h2)\n",
    "    h2 = act(h2)\n",
    "    h3 = nn.Conv(self.channels[2], (3, 3), (2, 2), padding='VALID', use_bias=False)(h2)\n",
    "    h3 += Dense(self.channels[2])(embed)\n",
    "    h3 = nn.GroupNorm()(h3)\n",
    "    h3 = act(h3)\n",
    "    h4 = nn.Conv(self.channels[3], (3, 3), (2, 2), padding='VALID', use_bias=False)(h3)\n",
    "    h4 += Dense(self.channels[3])(embed)\n",
    "    h4 = nn.GroupNorm()(h4)\n",
    "    h4 = act(h4)\n",
    "\n",
    "    # Decoding path\n",
    "    h = nn.Conv(self.channels[2], (3, 3), (1, 1), padding=((2, 2), (2, 2)), input_dilation=(2, 2), use_bias=False)(h4)\n",
    "    \n",
    "    ## Skip connection from the encoding path\n",
    "    h += Dense(self.channels[2])(embed)\n",
    "    h = nn.GroupNorm()(h)\n",
    "    h = act(h)\n",
    "    h = nn.Conv(self.channels[1], (3, 3), (1, 1), padding=((2, 3), (2, 3)),\n",
    "                  input_dilation=(2, 2), use_bias=False)(\n",
    "                      jnp.concatenate([h, h3], axis=-1)\n",
    "                  )\n",
    "    h += Dense(self.channels[1])(embed)\n",
    "    h = nn.GroupNorm()(h)\n",
    "    h = act(h)\n",
    "    h = nn.Conv(self.channels[0], (3, 3), (1, 1), padding=((2, 3), (2, 3)),\n",
    "                  input_dilation=(2, 2), use_bias=False)(\n",
    "                      jnp.concatenate([h, h2], axis=-1)\n",
    "                  )\n",
    "    h += Dense(self.channels[0])(embed)\n",
    "    h = nn.GroupNorm()(h)\n",
    "    h = act(h)\n",
    "    h = nn.Conv(1, (3, 3), (1, 1), padding=((2, 2), (2, 2)))(\n",
    "        jnp.concatenate([h, h1], axis=-1)\n",
    "    )\n",
    "\n",
    "    # Normalize output\n",
    "    h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecc1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up the SDE\n",
    "\n",
    "def marginal_prob_std(t, sigma):\n",
    "  \"\"\"Compute the mean and standard deviation of $p_{0t}(x(t) | x(0))$.\n",
    "\n",
    "  Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "  Returns:\n",
    "    The standard deviation.\n",
    "  \"\"\"\n",
    "  return jnp.sqrt((sigma**(2 * t) - 1.) / 2. / jnp.log(sigma))\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "  \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "  Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "  Returns:\n",
    "    The vector of diffusion coefficients.\n",
    "  \"\"\"\n",
    "  return sigma**t\n",
    "\n",
    "sigma =  25.0#@param {'type':'number'}\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "856742b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the loss function (double click to expand or collapse)\n",
    "\n",
    "def loss_fn(rng, model, params, x, marginal_prob_std, eps=1e-5):\n",
    "  \"\"\"The loss function for training score-based generative models.\n",
    "\n",
    "  Args:\n",
    "    model: A `flax.linen.Module` object that represents the structure of\n",
    "      the score-based model.\n",
    "    params: A dictionary that contains all trainable parameters.\n",
    "    x: A mini-batch of training data.\n",
    "    marginal_prob_std: A function that gives the standard deviation of\n",
    "      the perturbation kernel.\n",
    "    eps: A tolerance value for numerical stability.\n",
    "  \"\"\"\n",
    "  rng, step_rng = jax.random.split(rng)\n",
    "  random_t = jax.random.uniform(step_rng, (x.shape[0],), minval=eps, maxval=1.)\n",
    "  rng, step_rng = jax.random.split(rng)\n",
    "  z = jax.random.normal(step_rng, x.shape)\n",
    "  std = marginal_prob_std(random_t)\n",
    "  perturbed_x = x + z * std[:, None, None, None]\n",
    "  score = model.apply(params, perturbed_x, random_t)\n",
    "  loss = jnp.mean(jnp.sum((score * std[:, None, None, None] + z)**2,\n",
    "                          axis=(1,2,3)))\n",
    "  return loss\n",
    "\n",
    "def get_train_step_fn(model, marginal_prob_std):\n",
    "  \"\"\"Create a one-step training function.\n",
    "\n",
    "  Args:\n",
    "    model: A `flax.linen.Module` object that represents the structure of\n",
    "      the score-based model.\n",
    "    marginal_prob_std: A function that gives the standard deviation of\n",
    "      the perturbation kernel.\n",
    "  Returns:\n",
    "    A function that runs one step of training.\n",
    "  \"\"\"\n",
    "\n",
    "  val_and_grad_fn = jax.value_and_grad(loss_fn, argnums=2)\n",
    "  def step_fn(rng, x, optimizer):\n",
    "    params = optimizer.params\n",
    "    loss, grad = val_and_grad_fn(rng, model, params, x, marginal_prob_std)\n",
    "    \n",
    "    # mean_grad = jax.lax.pmean(grad, axis_name='device')\n",
    "    # mean_loss = jax.lax.pmean(loss, axis_name='device')\n",
    "    mean_loss =loss\n",
    "    mean_grad = grad\n",
    "    new_optimizer = optimizer.apply_gradient(mean_grad)\n",
    "\n",
    "    return mean_loss, new_optimizer\n",
    "  # return jax.pmap(step_fn, axis_name='device')\n",
    "  return step_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543a38818b2f450db88d4a3fd591fdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "split accepts a single key, but was given a key array of shape (2, 2) != (). Use jax.vmap for batching.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m rng, \u001b[38;5;241m*\u001b[39mstep_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng, jax\u001b[38;5;241m.\u001b[39mlocal_device_count() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m step_rng \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(step_rng)\n\u001b[0;32m---> 59\u001b[0m loss, state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m flax\u001b[38;5;241m.\u001b[39mjax_utils\u001b[38;5;241m.\u001b[39munreplicate(loss)\n\u001b[1;32m     61\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m, in \u001b[0;36mget_train_step_fn.<locals>.step_fn\u001b[0;34m(rng, x, optimizer)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_fn\u001b[39m(rng, x, optimizer):\n\u001b[1;32m     40\u001b[0m   params \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m---> 41\u001b[0m   loss, grad \u001b[38;5;241m=\u001b[39m \u001b[43mval_and_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarginal_prob_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m   \u001b[38;5;66;03m# mean_grad = jax.lax.pmean(grad, axis_name='device')\u001b[39;00m\n\u001b[1;32m     44\u001b[0m   \u001b[38;5;66;03m# mean_loss = jax.lax.pmean(loss, axis_name='device')\u001b[39;00m\n\u001b[1;32m     45\u001b[0m   mean_loss \u001b[38;5;241m=\u001b[39mloss\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(rng, model, params, x, marginal_prob_std, eps)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss_fn\u001b[39m(rng, model, params, x, marginal_prob_std, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"The loss function for training score-based generative models.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    eps: A tolerance value for numerical stability.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m   rng, step_rng \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m   random_t \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(step_rng, (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), minval\u001b[38;5;241m=\u001b[39meps, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m)\n\u001b[1;32m     17\u001b[0m   rng, step_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/random.py:290\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msplit\u001b[39m(key: ArrayLike, num: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Splits a PRNG key into `num` new keys by adding a leading axis.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    An array-like object of `num` new PRNG keys.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m   typed_key, wrapped \u001b[38;5;241m=\u001b[39m \u001b[43m_check_prng_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(wrapped, _split(typed_key, num))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/random.py:99\u001b[0m, in \u001b[0;36m_check_prng_key\u001b[0;34m(name, key, allow_batched)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munexpected PRNG key type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_batched) \u001b[38;5;129;01mand\u001b[39;00m wrapped_key\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m---> 99\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accepts a single key, but was given a key array of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_key, wrapped\n",
      "\u001b[0;31mValueError\u001b[0m: split accepts a single key, but was given a key array of shape (2, 2) != (). Use jax.vmap for batching."
     ]
    }
   ],
   "source": [
    "#@title Training (double click to expand or collapse)\n",
    "import functools\n",
    "import flax\n",
    "from flax.serialization import to_bytes, from_bytes\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from tqdm import notebook as tqdm\n",
    "\n",
    "n_epochs = 50#@param {'type':'integer'}\n",
    "## size of a mini-batch\n",
    "batch_size = 256 #@param {'type':'integer'}\n",
    "## learning rate\n",
    "lr=1e-4 #@param {'type':'number'}\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "fake_input = jnp.ones((batch_size, 28, 28, 1))\n",
    "fake_time = jnp.ones(batch_size)\n",
    "score_model = ScoreNet(marginal_prob_std_fn)\n",
    "params = score_model.init({'params': rng}, fake_input, fake_time)\n",
    "\n",
    "# Load MNIST dataset using TensorFlow\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1] and add channel dimension\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_train = x_train[..., tf.newaxis]  # Add channel dimension (28, 28, 1)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create optimizer and training state\n",
    "tx = optax.adam(learning_rate=lr)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=score_model.apply,\n",
    "    params=params['params'],\n",
    "    tx=tx\n",
    ")\n",
    "train_step_fn = get_train_step_fn(score_model, marginal_prob_std_fn)\n",
    "tqdm_epoch = tqdm.trange(n_epochs)\n",
    "\n",
    "assert batch_size % jax.local_device_count() == 0\n",
    "data_shape = (jax.local_device_count(), -1, 28, 28, 1)\n",
    "\n",
    "state = flax.jax_utils.replicate(state)\n",
    "for epoch in tqdm_epoch:\n",
    "  avg_loss = 0.\n",
    "  num_items = 0\n",
    "  for x, y in dataset:\n",
    "    # Convert TensorFlow tensor to numpy and reshape for JAX\n",
    "    x = x.numpy().reshape(data_shape)\n",
    "    rng, *step_rng = jax.random.split(rng, jax.local_device_count() + 1)\n",
    "    step_rng = jnp.asarray(step_rng)\n",
    "    loss, state = train_step_fn(step_rng, x, state)\n",
    "    loss = flax.jax_utils.unreplicate(loss)\n",
    "    avg_loss += loss.item() * x.shape[0]\n",
    "    num_items += x.shape[0]\n",
    "  # Print the averaged training loss so far.\n",
    "  tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "  # Update the checkpoint after each epoch of training.\n",
    "  with tf.io.gfile.GFile('ckpt.flax', 'wb') as fout:\n",
    "    fout.write(to_bytes(flax.jax_utils.unreplicate(state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ccea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.local_device_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
