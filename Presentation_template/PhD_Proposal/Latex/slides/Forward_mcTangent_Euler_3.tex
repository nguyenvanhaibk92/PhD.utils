\frame{\frametitle{Analysis: Data randomization}
    % \bluebox{randomized inputs}{
    % \[\ubr = \ub + \epsb,\]
    % where $\epsb$ is a normal random vector $\epsb \sim \mc{N}\LRp{\bs{0}, \delta^2 \Ib}$
    % }

\bluebox{Loss function (sum over all dataset windows)}{
\[
\mc{L} := \frac{1}{N_t} \sum_{i=0}^{N_t - 1} \MSE{\ui{i,1} - \ut{i,1}} + \alpha \MSE{\ubar{i,1} - \ut{i,1}}
\]
}

\vspace{2ex}

\orangebox{Randomized loss function}{
    Denoting randomized version of $\ui{i,0}$ is
    \begin{equation*}
        \vi{i,0} = \ui{i,0} + \epsb
    \end{equation*}
    where, $\epsb$ is noise vector. The randomized loss function is
    \begin{equation*}
        %\mc{L}^{\text{rand}}\LRp{\vi{i,0}} = \frac{1}{N_t} \sum_{i=0}^{N_t - 1} \LRs{\mc{L}_{\text{ML}} \LRp{\vi{i,0} } + \alpha \mc{L}_{\text{MC}}\LRp{\vi{i,0}}}
        \mc{L}^{\text{rand}} =  \frac{1}{N_t} \sum_{i=0}^{N_t - 1} \MSE{\vi{i,1} - \vt{i,1}} + \alpha \MSE{\vbar{i,1} - \vt{i,1}}
    \end{equation*}
    % where
    % \begin{equation*}
    %     % \begin{aligned}
    %     %     {\mc{L}_{\text{ML}}\LRp{\ui{i,0}}} = & \MSE{\ui{i,1} - \ut{i,1}} = \MSE{\ui{i,1} - \LRp{\ui{i,0} + \dt \NN{\ui{i,0}}}} \\
    %     %      \mc{L}_{\text{MC}}\LRp{\ui{i,0}} = & \MSE{\ubar{i,1} - \ut{i,1}} = \dt^2 \MSE{ \G\LRp{\ui{i,0}} - \NN{\ui{i,0}}}
    %     % \end{aligned}
    %     \begin{aligned}
    %         \bullet & \textcolor{green}{\text{ Machine learning: }} &  & {\mc{L}_{\text{ML}}\LRp{\vi{i,0}}} = \MSE{\vi{i,1} - \vt{i,1}} \\
    %         \bullet & \textcolor{blue}{\text{ Model-constrained: }} &  & \mc{L}_{\text{MC}}\LRp{\vi{i,0}} = \MSE{\vbar{i,1} - \vt{i,1}}
    %     \end{aligned}
    % \end{equation*}
}
}
